# **Bijou-Core v1 â€” On-Device Command Model for Wearables**

Bijou-Core is a tiny, on-device language model designed to convert **speech â†’ device actions** instantly.

It runs fully offline, optimized for  **headphones, wearables, AR glasses, and low-power hardware** .

This repository contains the  **MVP implementation** , including:

* synthetic dataset generation
* tool-schema definitions
* small-model fine-tuning
* schema-constrained decoding
* quantized inference engine
* browser-based demo (mic â†’ STT â†’ model â†’ action)

---

# ğŸš€ **What We're Building**

Wearables today rely on cloud LLMs â†’ slow, wrong, delayed.

Bijou-Core fixes this by using a **tiny, specialized model** that only does one thing:

> **Understand a userâ€™s command and trigger the correct function.**

Example:

User says:

> â€œturn noise cancelling to highâ€

Bijou-Core outputs:

```json
{
  "function": "set_anc_mode",
  "mode": "high"
}
```

Zero hallucination.

Zero chit-chat.

Just actions.

---

# ğŸ§  **Architecture Overview**

```
Microphone
    â†“
Audio Preprocessing (VAD, noise filtering)
    â†“
Speech-to-Text (Whisper Tiny / Bijou-STT)
    â†“
Bijou-Core (Small Command Model)
    â†“
Tool-Calling Schema Engine
    â†“
Device Action Layer (simulator or OEM SDK)
```

---

# ğŸ”§ **Features in the MVP**

### âœ” Synthetic dataset generator

Generates command â†’ function-call pairs using a teacher model (Qwen/Phi/etc.).

### âœ” Tool-schema definition (`tools.json`)

Defines the full list of actions a target device supports.

### âœ” Fine-tuning for tool-calling

Train small models (1â€“4B) to output  **structured JSON only** .

### âœ” Schema-constrained decoding

Ensures every output is valid, typed, and deterministic.

### âœ” Quantized inference

Export to **int8/int4** for fast local inference.

### âœ” Wearable Simulator (Browser Demo)

Mic â†’ STT â†’ LLM â†’ JSON â†’ simulated device UI

(Used for testing and demos).

---

# ğŸ› ï¸ **Repository Structure**

```
bijou-core/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                 # synthetic text before processing
â”‚   â”œâ”€â”€ processed/           # final datasets
â”‚   â””â”€â”€ generators/          # synthetic dataset scripts
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ base/                # chosen starting model (Qwen/Phi/Gemma)
â”‚   â”œâ”€â”€ finetuned/           # model after command training
â”‚   â””â”€â”€ quantized/           # int4/int8 export
â”‚
â”œâ”€â”€ tools/
â”‚   â”œâ”€â”€ tools.json           # OEM-specific action schema
â”‚   â”œâ”€â”€ schema_engine/       # JSON validation, repair logic
â”‚   â””â”€â”€ adapters/            # LoRA fine-tunes per skill pack
â”‚
â”œâ”€â”€ inference/
â”‚   â”œâ”€â”€ engine.cpp           # Rust inference engine (quantized)
â”‚   â””â”€â”€ runtime/             # wrappers, tokenization, kernels
â”‚
â”œâ”€â”€ demo/
â”‚   â”œâ”€â”€ web/                 # browser demo (mic â†’ model â†’ action)
â”‚   â””â”€â”€ simulator/           # UI simulating target OEM device
â”‚
â””â”€â”€ scripts/
    â”œâ”€â”€ generate_data.py     # synthetic dataset generator
    â”œâ”€â”€ finetune.py          # FT on command mapping
    â”œâ”€â”€ quantize.py          # int4/int8 export
    â””â”€â”€ evaluate.py          # accuracy + latency tests
```

---

# ğŸ“¦ **Installation (MVP)**

Clone repo:

```bash
git clone https://github.com/your-org/bijou-core
cd bijou-core
```

Install dependencies:

```bash
pip install -r requirements.txt
```

Download base model:

```bash
python scripts/download_base_model.py --model qwen2.5-1.5b
```

---

# ğŸ”¨ **How to Run the MVP**

## **1. Define device actions**

Edit `tools/tools.json`:

```json
{
  "tools": [
    {"name": "set_volume", "params": {"level": "int"}},
    {"name": "set_anc_mode", "params": {"mode": ["off","low","high"]}}
  ]
}
```

---

## **2. Generate synthetic dataset**

```bash
python scripts/generate_data.py \
    --tools tools/tools.json \
    --output data/processed/omi_dataset.jsonl
```

---

## **3. Fine-tune model**

```bash
python scripts/finetune.py \
    --model models/base/qwen2.5-1.5b \
    --data data/processed/omi_dataset.jsonl
```

---

## **4. Quantize to int4**

```bash
python scripts/quantize.py \
    --model models/finetuned/bijou-core-mvp \
    --output models/quantized/bijou-core-int4
```

---

## **5. Run local browser demo**

```bash
cd demo/web
npm install
npm run dev
```

Open the UI, speak into your microphone, and watch the model:

* detect your command
* output structured JSON
* trigger simulated device actions

---

# ğŸ§ª **Evaluation**

Run:

```bash
python scripts/evaluate.py \
    --model models/quantized/bijou-core-int4
```

Evaluates:

* tool-calling accuracy
* schema validity
* noise robustness
* latency

---

# ğŸ—ºï¸ **Roadmap**

### **v1 (MVP)**

* STT â†’ Bijou-Core â†’ JSON output
* Web simulator
* OEM-targeted dataset
* Fine-tuning small base models
* int4 quantization

### **v2 (Production Candidate)**

* Distilled <700M Bijou-Core
* On-device DSP/NNAPI acceleration
* Skill Packs (LoRA)
* Multilingual command support

### **v3 (OEM Release)**

* Partner integrations
* Offline multimodal conditioning
* Hybrid cloud fallback
* Full embedded SDK

---

# ğŸ¤ **License**

MIT (MVP) â€” subject to change for OEM licensing.

---

# ğŸ“ **Contact**

If youâ€™re a hardware company interested in partnering:

**email:** [founders@bijou.ai](mailto:founders@bijou.ai)

**twitter:** @your_handle
