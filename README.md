<img width="1825" height="270" alt="image" src="https://github.com/user-attachments/assets/99625444-ebc7-48e1-af4b-203ce4c82374" />
<p align="center">
    <b>Tiny language models for tiny devices</b>
</p>

Bijou is a framework and series tiny, on-device language models designed to work for minute expert tasks.

It runs fully offline, optimized for  **headphones, wearables, AR glasses, and low-power hardware** .

This repository contains:

* synthetic dataset generation
* tool-schema definitions
* small-model fine-tuning
* schema-constrained decoding
* quantized inference engine
* browser-based demo (mic â†’ STT â†’ model â†’ action)

---

# ğŸš€ **What We're Building**

Wearables today rely on cloud LLMs â†’ slow, wrong, delayed.

Bijou-Core fixes this by using a **tiny, specialized model** that only does one thing:

> **Understand a userâ€™s command and trigger the correct function.**

Example:

User says:

> â€œturn noise cancelling to highâ€

Bijou-Core outputs:

```json
{
  "function": "set_anc_mode",
  "mode": "high"
}
```

Zero hallucination.

Zero chit-chat.

Just actions.

---

# ğŸ› ï¸ **Repository Structure**

```
bijou/
â”‚
â”œâ”€â”€ adapters/           # LoRA adapters & skill-pack modules for extending model capabilities
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ api/                # Public API interfaces (Python/JS) for calling the model + schema engine
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ data/               # Synthetic + processed datasets used for fine-tuning Bijou-Core
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ demo/               # Browser/desktop demo (mic â†’ STT â†’ model â†’ action simulator)
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ inference/             # On-device inference engine (quantized models, kernels, runtime)
â”‚   â””â”€â”€ README.md
|
â”œâ”€â”€ models/             # Base, fine-tuned, and quantized model checkpoints
â”‚   â”œâ”€â”€ base/           # Original downloaded SLMs (Qwen, Phi, Gemma, etc.)
â”‚   â”œâ”€â”€ finetuned/      # Command-specialized models trained for tool-calling
â”‚   â”œâ”€â”€ quantized/      # int4/int8 optimized exports for on-device inference
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ OEMs/               # OEM-specific configs (schemas, notes, device constraints)
â”‚   â”œâ”€â”€ omi/            # Example target OEM folder with tools.json + integration notes
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ scripts/            # Training, dataset generation, quantization, and evaluation scripts
â”‚   â””â”€â”€ README.md
â”‚
â””â”€â”€ utils/              # Shared utilities (tokenization, schema validation, helpers)
    â””â”€â”€ README.md

```

---


# ğŸ¤ **License**

MIT (MVP) â€” subject to change for OEM licensing.
