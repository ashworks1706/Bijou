# **Bijou-Core v1 â€” On-Device Command Model for Wearables**

Bijou-Core is a tiny, on-device language model designed to convert **speech â†’ device actions** instantly.

It runs fully offline, optimized for  **headphones, wearables, AR glasses, and low-power hardware** .

This repository contains the  **MVP implementation** , including:

* synthetic dataset generation
* tool-schema definitions
* small-model fine-tuning
* schema-constrained decoding
* quantized inference engine
* browser-based demo (mic â†’ STT â†’ model â†’ action)

---

# ğŸš€ **What We're Building**

Wearables today rely on cloud LLMs â†’ slow, wrong, delayed.

Bijou-Core fixes this by using a **tiny, specialized model** that only does one thing:

> **Understand a userâ€™s command and trigger the correct function.**

Example:

User says:

> â€œturn noise cancelling to highâ€

Bijou-Core outputs:

```json
{
  "function": "set_anc_mode",
  "mode": "high"
}
```

Zero hallucination.

Zero chit-chat.

Just actions.

---

# ğŸ§  **Architecture Overview**

```
Microphone
    â†“
Audio Preprocessing (VAD, noise filtering)
    â†“
Speech-to-Text (Whisper Tiny / Bijou-STT)
    â†“
Bijou-Core (Small Command Model)
    â†“
Tool-Calling Schema Engine
    â†“
Device Action Layer (simulator or OEM SDK)
```

---

# ğŸ”§ **Features in the MVP**

### âœ” Synthetic dataset generator

Generates command â†’ function-call pairs using a teacher model (Qwen/Phi/etc.).

### âœ” Tool-schema definition (`tools.json`)

Defines the full list of actions a target device supports.

### âœ” Fine-tuning for tool-calling

Train small models (1â€“4B) to output  **structured JSON only** .

### âœ” Schema-constrained decoding

Ensures every output is valid, typed, and deterministic.

### âœ” Quantized inference

Export to **int8/int4** for fast local inference.

### âœ” Wearable Simulator (Browser Demo)

Mic â†’ STT â†’ LLM â†’ JSON â†’ simulated device UI

(Used for testing and demos).

---

# ğŸ› ï¸ **Repository Structure**

```
bijou/
â”‚
â”œâ”€â”€ adapters/           # LoRA adapters & skill-pack modules for extending model capabilities
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ api/                # Public API interfaces (Python/JS) for calling the model + schema engine
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ data/               # Synthetic + processed datasets used for fine-tuning Bijou-Core
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ demo/               # Browser/desktop demo (mic â†’ STT â†’ model â†’ action simulator)
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ inference/             # On-device inference engine (quantized models, kernels, runtime)
â”‚   â””â”€â”€ README.md
|
â”œâ”€â”€ models/             # Base, fine-tuned, and quantized model checkpoints
â”‚   â”œâ”€â”€ base/           # Original downloaded SLMs (Qwen, Phi, Gemma, etc.)
â”‚   â”œâ”€â”€ finetuned/      # Command-specialized models trained for tool-calling
â”‚   â”œâ”€â”€ quantized/      # int4/int8 optimized exports for on-device inference
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ OEMs/               # OEM-specific configs (schemas, notes, device constraints)
â”‚   â”œâ”€â”€ omi/            # Example target OEM folder with tools.json + integration notes
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ scripts/            # Training, dataset generation, quantization, and evaluation scripts
â”‚   â””â”€â”€ README.md
â”‚
â””â”€â”€ utils/              # Shared utilities (tokenization, schema validation, helpers)
    â””â”€â”€ README.md

```

---

# ğŸ“¦ **Installation (MVP)**

Clone repo:

```bash
git clone https://github.com/your-org/bijou-core
cd bijou-core
```

Install dependencies:

```bash
pip install -r requirements.txt
```

Download base model:

```bash
python scripts/download_base_model.py --model qwen2.5-1.5b
```

---

# ğŸ”¨ **How to Run the MVP**

## **1. Define device actions**

Edit `tools/tools.json`:

```json
{
  "tools": [
    {"name": "set_volume", "params": {"level": "int"}},
    {"name": "set_anc_mode", "params": {"mode": ["off","low","high"]}}
  ]
}
```

---

## **2. Generate synthetic dataset**

```bash
python scripts/generate_data.py \
    --tools tools/tools.json \
    --output data/processed/omi_dataset.jsonl
```

---

## **3. Fine-tune model**

```bash
python scripts/finetune.py \
    --model models/base/qwen2.5-1.5b \
    --data data/processed/omi_dataset.jsonl
```

---

## **4. Quantize to int4**

```bash
python scripts/quantize.py \
    --model models/finetuned/bijou-core-mvp \
    --output models/quantized/bijou-core-int4
```

---

## **5. Run local browser demo**

```bash
cd demo/web
npm install
npm run dev
```

Open the UI, speak into your microphone, and watch the model:

* detect your command
* output structured JSON
* trigger simulated device actions

---

# ğŸ§ª **Evaluation**

Run:

```bash
python scripts/evaluate.py \
    --model models/quantized/bijou-core-int4
```

Evaluates:

* tool-calling accuracy
* schema validity
* noise robustness
* latency

---

# ğŸ—ºï¸ **Roadmap**

### **v1 (MVP)**

* STT â†’ Bijou-Core â†’ JSON output
* Web simulator
* OEM-targeted dataset
* Fine-tuning small base models
* int4 quantization

### **v2 (Production Candidate)**

* Distilled <700M Bijou-Core
* On-device DSP/NNAPI acceleration
* Skill Packs (LoRA)
* Multilingual command support

### **v3 (OEM Release)**

* Partner integrations
* Offline multimodal conditioning
* Hybrid cloud fallback
* Full embedded SDK

---

# ğŸ¤ **License**

MIT (MVP) â€” subject to change for OEM licensing.
